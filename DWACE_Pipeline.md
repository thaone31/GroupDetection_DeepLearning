## ğŸ”„ DWACE Pipeline Documentation
### DeepWalk AutoEncoder Contrastive Enhancement

## ğŸ“Š Pipeline Flow Diagram

```
Input Graph G
     â†“
ğŸ“Š Step 1: Base Embedding Generation
     â”œâ”€â”€ DeepWalk Embedding (dim=128)
     â””â”€â”€ Node2Vec Embedding (dim=128)
     â†“
ğŸ”§ Step 2: AutoEncoder Processing (if needed)
     â†“ (if embedding_dim > target_dim)
   AutoEncoder Dimensionality Reduction
   - Graph-aware loss (modularity + neighborhood)
   - Supervised classifier head (if ground truth)
     â†“
   deepwalk_ae
     â†“
ğŸ”€ Step 3: Enhancement Branch Selection
     â†“
   Ground Truth Available?
     â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   âœ… YES                           âŒ NO
   (Karate, Football)               (Dolphins, Email)
   â†“                                â†“
ğŸ“Š Contrastive Learning Branch      â¹ï¸ No Additional Enhancement
   â”œâ”€â”€ Data Augmentation            â”œâ”€â”€ Use deepwalk_ae as final
   â”‚   - Feature Dropout (10-15%)   â”‚   embedding
   â”‚   - Gaussian Noise (Ïƒ=0.05)    â””â”€â”€ Proceed to GAE-GCN
   â”‚   - Feature Shuffling (5%)     
   â”œâ”€â”€ InfoNCE Loss                 
   â”œâ”€â”€ Supervised Loss              
   â””â”€â”€ Gradient Clipping            
   â†“                                â†“
deepwalk_ae_contrast              deepwalk_ae (final)
   â†“                                â†“
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
              ğŸ¯ GAE-GCN Clustering
                     â†“
              ğŸ“ˆ Performance Metrics
```

## ğŸ¯ Branch Selection Logic

### âœ… Contrastive Learning Branch (Supervised)
**Used when**: Ground truth labels available
**Datasets**: Karate Club, Football
**Key Features**:
- Multi-view contrastive learning
- InfoNCE loss with improved numerical stability
- Supervised classification loss
- Advanced data augmentation
- Learning rate scheduling
- Gradient clipping

**Architecture**:
```
Input â†’ Dense(256) â†’ BatchNorm â†’ Dropout(0.3) â†’
Dense(128) â†’ BatchNorm â†’ Dropout(0.2) â†’
Dense(64) â†’ LayerNorm â†’ [Projection Head | Classifier]
```

### âŒ No Enhancement Branch (Unsupervised)
**Used when**: No ground truth available
**Datasets**: Dolphins, Email, Facebook, etc.
**Key Features**:
- Uses `deepwalk_ae` embedding directly
- No additional enhancement layer
- Simpler and more stable for unsupervised scenarios
- Avoids overfitting on unlabeled data

**Architecture**:
```
deepwalk_ae â†’ GAE-GCN Clustering
```

## ğŸ”§ Implementation Details

### Hyperparameters

**Contrastive Learning**:
- Temperature: 0.05
- Lambda contrastive: 1.0
- Lambda supervised: 0.3
- Epochs: 150
- Learning rate: 0.001 â†’ decay 5% per 10 epochs

**Unsupervised (No Enhancement)**:
- Uses deepwalk_ae directly
- No additional hyperparameters
- Faster training and inference

**AutoEncoder (Base)**:
- Hidden dim: max(out_dim * 2, 32)
- Lambda reconstruction: 1.0
- Lambda modularity: 0.1
- Lambda neighborhood: 0.1
- Lambda supervised: 1.0

### Data Augmentation (Contrastive Branch)
1. **Feature Dropout**: 10-15% random feature masking
2. **Gaussian Noise**: Ïƒ = 0.05-0.08
3. **Feature Shuffling**: 5% of features, 30% probability

## ğŸ“ˆ Performance Comparison

### Karate Club (Supervised)
- **deepwalk_ae_contrast** performs best
- ARI: ~0.60, NMI: ~0.67
- Leverages ground truth effectively

### Dolphins (Unsupervised)
- **deepwalk_ae** and **deepwalk_ae_vae** competitive
- **deepwalk_ae_vae** shows consistent performance
- Modularity: ~0.48-0.50

## ğŸš€ Usage Example

```python
# The pipeline automatically selects the appropriate branch
embeddings_dict, enhancement_name = dwace_pipeline(
    G=graph,
    ground_truth=labels,  # None for unsupervised
    feature_dim=128,
    walk_params={'num_walks': 20, 'walk_length': 40}
)

# Results in either:
# enhancement_name = "deepwalk_ae_contrast" (supervised)
# enhancement_name = "deepwalk_ae_vae" (unsupervised)
```

## ğŸ¯ Key Benefits

1. **Adaptive**: Automatically selects optimal enhancement strategy
2. **Robust**: Works well for both supervised and unsupervised scenarios  
3. **Scalable**: Efficient implementation with memory management
4. **Consistent**: Unified interface regardless of data type
5. **State-of-the-art**: Incorporates latest advances in contrastive learning and VAE
